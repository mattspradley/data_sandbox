{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a5a4a7",
   "metadata": {},
   "source": [
    "# Data Exploration Template\n",
    "\n",
    "This notebook provides a template for exploring new datasets.\n",
    "\n",
    "**Dataset:** [Replace with your dataset name]\n",
    "**Date:** [Replace with current date]\n",
    "**Author:** [Your name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa23cb",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689b2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85f7f9",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset here\n",
    "# Example: df = pd.read_csv('../data/raw/your_dataset.csv')\n",
    "\n",
    "# For demonstration, let's create a sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 65, n_samples),\n",
    "    'income': np.random.normal(50000, 15000, n_samples),\n",
    "    'score': np.random.uniform(0, 100, n_samples),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples),\n",
    "    'is_active': np.random.choice([True, False], n_samples, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "# Introduce some missing values\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * n_samples), replace=False)\n",
    "df.loc[missing_indices, 'income'] = np.nan\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc171669",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n=== MEMORY USAGE ===\")\n",
    "print(df.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "print(\"=== FIRST 5 ROWS ===\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n=== LAST 5 ROWS ===\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=== STATISTICAL SUMMARY ===\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57584143",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc32a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"=== DUPLICATE ROWS ===\")\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    display(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c81da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and unique values\n",
    "print(\"=== UNIQUE VALUES PER COLUMN ===\")\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")\n",
    "    \n",
    "    # Show unique values for categorical columns\n",
    "    if df[col].dtype == 'object' or unique_count < 10:\n",
    "        print(f\"  Values: {sorted(df[col].unique())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b00f1d",
   "metadata": {},
   "source": [
    "## 5. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical variables\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numerical_cols) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols[:4]):\n",
    "        if i < len(axes):\n",
    "            df[col].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(len(numerical_cols), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns found for distribution plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36538c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical variables\n",
    "if len(numerical_cols) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 numerical columns for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03667f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables visualization\n",
    "categorical_cols = df.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    fig, axes = plt.subplots(1, min(len(categorical_cols), 3), figsize=(15, 5))\n",
    "    if len(categorical_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols[:3]):\n",
    "        if i < len(axes):\n",
    "            value_counts = df[col].value_counts()\n",
    "            value_counts.plot(kind='bar', ax=axes[i], alpha=0.7)\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No categorical columns found for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f762c",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc97769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "if len(numerical_cols) > 0:\n",
    "    fig, axes = plt.subplots(1, min(len(numerical_cols), 4), figsize=(16, 4))\n",
    "    if len(numerical_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols[:4]):\n",
    "        if i < len(axes):\n",
    "            df.boxplot(column=col, ax=axes[i])\n",
    "            axes[i].set_title(f'Box Plot: {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical outlier detection using IQR\n",
    "    print(\"=== OUTLIER DETECTION (IQR Method) ===\")\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / len(df)) * 100\n",
    "        \n",
    "        print(f\"{col}: {outlier_count} outliers ({outlier_percentage:.2f}%)\")\n",
    "        if outlier_count > 0:\n",
    "            print(f\"  Range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "            print(f\"  Outlier values: {sorted(outliers[col].values)[:10]}...\" if outlier_count > 10 else f\"  Outlier values: {sorted(outliers[col].values)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No numerical columns found for outlier detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339c574",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Handle missing values if any\")\n",
    "print(\"2. Remove or investigate outliers\")\n",
    "print(\"3. Feature engineering if needed\")\n",
    "print(\"4. Prepare data for modeling\")\n",
    "print(\"5. Save cleaned data to '../data/processed/'\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDED ACTIONS ===\")\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"• Address missing values\")\n",
    "if df.duplicated().sum() > 0:\n",
    "    print(\"• Remove duplicate rows\")\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"• Consider scaling numerical features\")\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"• Encode categorical variables if needed\")\n",
    "\n",
    "print(\"\\nExploration complete! 🎉\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe706a5",
   "metadata": {},
   "source": [
    "## 8. Save Processed Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002918d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the lines below to save your processed data\n",
    "\n",
    "# # Create a copy for processing\n",
    "# df_processed = df.copy()\n",
    "\n",
    "# # Apply any cleaning steps here\n",
    "# # Example: df_processed = df_processed.dropna()\n",
    "# # Example: df_processed = df_processed.drop_duplicates()\n",
    "\n",
    "# # Save to processed data folder\n",
    "# import os\n",
    "# os.makedirs('../data/processed', exist_ok=True)\n",
    "# df_processed.to_csv('../data/processed/cleaned_dataset.csv', index=False)\n",
    "# print(\"Processed data saved to '../data/processed/cleaned_dataset.csv'\")\n",
    "\n",
    "print(\"Data processing template ready for your customization!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
